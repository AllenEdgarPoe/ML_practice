{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형 회귀(최소제곱법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import mglearn\n",
    "\n",
    "X,y = mglearn.datasets.make_wave(n_samples=60)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44153666]\n",
      "-0.01711124414733381\n",
      "train accuracy: 0.6592061059587275\n",
      "test accuracy: 0.6932519118518163\n"
     ]
    }
   ],
   "source": [
    "practice = LinearRegression().fit(X_train, y_train)\n",
    "print(practice.coef_)  #기울기의 파라미터, 즉 weight의미, 여기서는 X의 feature이 하나 이므로 coef_도 하나로 나옴\n",
    "print(practice.intercept_) #편향, bias\n",
    "print('train accuracy: {}'.format(practice.score(X_train, y_train)))\n",
    "print('test accuracy: {}'.format(practice.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy가 낮은 편이다. 하지만 test 정확도와 train 정확도가 비슷한 것으로 보아 과소적합된 것을 알 수 있다. 이 경우 feature의 갯수가 너무 적기 때문에 일어난 것이다.\n",
    "(행렬에서 조건이 변수보다 많을때 조건을 모두 충족하는 변수를 찾기 힘든 것처럼..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 복잡한 데이터에서 식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 104)\n"
     ]
    }
   ],
   "source": [
    "X, y = mglearn.datasets.load_extended_boston()\n",
    "print(X.shape)   #꽤 복잡한 데이터이라는 것을 알 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9281262443260945\n",
      "test accuracy: 0.8962092351509607\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 1)\n",
    "boston = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('train accuracy: {}'.format(boston.score(X_train, y_train)))\n",
    "print('test accuracy: {}'.format(boston.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting을 줄이기 위한 Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**릿지(Ridge) 회귀** <br>L2 규제를 사용하여 과대적합을 막음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.8406599011793963\n",
      "test accuracy: 0.8836280810371688\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "print('train accuracy: {}'.format(ridge.score(X_train,y_train)))\n",
    "print('test accuracy: {}'.format(ridge.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또 다른 방법은 alpha(learning rate) 값을 줄이는 것. 계수에 대한 제약이 풀리면서 정확도는 줄어들지만 정확도는 커진다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.41091241e+02 -4.89988810e+00 -6.30016655e+01  1.19613502e+01\n",
      " -5.24626962e+00  9.96397131e+01  4.67677818e+01 -3.99133703e+01\n",
      "  1.87656969e+01  3.42078734e+01  7.93073960e+00  3.91787065e+01\n",
      "  2.70703623e+01  2.13778047e+01  1.33800827e+03  3.19887289e+02\n",
      "  2.06705158e+02 -2.28569053e+01  4.91676336e+01 -3.07271052e+01\n",
      " -2.28532847e+01 -1.59958078e+01 -3.21335873e+02  2.57816736e+02\n",
      " -1.00073255e+01  5.73923006e+01 -8.56306444e+00 -1.97335532e+01\n",
      " -1.33382261e+01 -6.44706136e+01  1.33388891e+01  5.31012457e+00\n",
      " -1.53341101e+01 -3.01867843e+00  4.33051219e+01 -5.39579262e+00\n",
      "  1.61648685e+01 -1.70474624e+01  2.29673613e+01 -6.75860883e+00\n",
      "  3.84013021e+01  3.98626764e+01  6.19358616e+00  5.17782999e+01\n",
      " -4.89946281e+01  2.63873305e+01 -4.31659746e+00  8.81679294e+00\n",
      " -1.02963088e+01  1.19613502e+01 -2.46527746e+01 -2.96373831e+01\n",
      " -4.04711034e-01  1.47458169e+01 -1.74968434e+01  3.13284574e+01\n",
      " -1.44075248e+01  6.60910261e+00 -8.91065375e+00 -2.81005650e+00\n",
      "  1.42066481e+01 -2.76496531e+01  1.01186026e+02 -5.25600653e+00\n",
      "  1.63297682e+01 -5.43014353e+01  3.24080663e+00  2.03062063e+01\n",
      "  1.86827682e+00 -4.85927177e+01 -1.10028513e+01  1.97217845e+00\n",
      " -6.66999872e+01 -2.48790904e+01 -1.03417259e+01 -3.12448696e+01\n",
      "  6.01972757e+00 -2.08729834e+00  2.93891430e+01 -1.74564504e+01\n",
      " -1.25977462e+00 -2.25975639e+01 -4.04169378e+01  6.90560801e+01\n",
      " -4.62805622e+01 -2.97491969e+01 -1.80377585e+01 -2.19904894e+01\n",
      "  2.10636831e+01 -8.17424126e+01  1.25101415e+02  4.61057966e-01\n",
      " -6.61205205e-01 -1.61026081e+01 -3.17119145e+01  3.06236433e+01\n",
      " -1.95218872e+01 -2.64958732e+01 -6.72345054e-01  1.45362451e+01\n",
      "  8.71993346e+00 -4.70280951e+00 -9.93047740e+00  2.27697408e+01]\n",
      "-40.67488629681235\n",
      "train accuracy: 0.9281262443260945\n",
      "test accuracy: 0.8962092351509607\n"
     ]
    }
   ],
   "source": [
    "practice = LinearRegression(0.2).fit( X_train, y_train)\n",
    "print(practice.coef_)  #기울기의 파라미터, 즉 weight의미, 여기서는 X의 feature이 하나 이므로 coef_도 하나로 나옴\n",
    "print(practice.intercept_) #편향, bias\n",
    "print('train accuracy: {}'.format(practice.score(X_train, y_train)))\n",
    "print('test accuracy: {}'.format(practice.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso 회귀**<br>L1 규제를 사용하여 계수의 절댓값을 0에 가깝게 만든다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.2391823851442012\n",
      "test accuracy: 0.20215528139551964\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso().fit(X_train, y_train)\n",
    "print('train accuracy: {}'.format(lasso.score(X_train, y_train)))\n",
    "print('test accuracy: {}'.format(lasso.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso 는 feature selection을 자동으로 해주는 것. 특성이 많은데 그 중 일부만 중요한 데이터면 Lasso로 하는게 낫다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
